{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XktEHi43twa5"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTiFk1gVEFem",
        "outputId": "b0fe7b16-2d75-47a0-b9bc-cef33e90fc33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-8JYDwQaKFx",
        "outputId": "9ea35a01-ee25-45bd-c727-72e3d90a629e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Collecting pyaml>=16.9 (from scikit-optimize)\n",
            "  Downloading pyaml-24.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.2.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-24.4.0 scikit-optimize-0.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK5GdH9mEhJr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/UTMW3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxfr2zREAmX2",
        "outputId": "1104aa0d-02ab-44b6-e1b9-938af9812799"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   proof  probability  finite-automata  np-complete  \\\n",
            "0      0            0                0            0   \n",
            "1      0            0                0            0   \n",
            "2      0            0                0            0   \n",
            "3      0            0                0            0   \n",
            "4      0            0                0            0   \n",
            "\n",
            "   mathematical-optimization  linear-programming  satisfiability  \\\n",
            "0                          0                   0               0   \n",
            "1                          0                   0               0   \n",
            "2                          0                   0               0   \n",
            "3                          0                   0               0   \n",
            "4                          0                   0               0   \n",
            "\n",
            "   stable-marriage  np  complexity-theory  ...  list  singly-linked-list  \\\n",
            "0                0   0                  0  ...     0                   0   \n",
            "1                0   0                  0  ...     0                   0   \n",
            "2                0   0                  0  ...     2                   0   \n",
            "3                0   0                  0  ...     0                   0   \n",
            "4                0   0                105  ...   518                   0   \n",
            "\n",
            "   disjoint-sets  amortized-analysis  set-intersection  set-union  set-theory  \\\n",
            "0              0                   0                 0          0           0   \n",
            "1              0                   0                 0          0           0   \n",
            "2              0                   0                 0          0           0   \n",
            "3              0                   0                 0          0           0   \n",
            "4              0                   0                 0          0           0   \n",
            "\n",
            "   set-difference  set-operations  disjoint-union  \n",
            "0               0               0               0  \n",
            "1               0               0               0  \n",
            "2               0               0               0  \n",
            "3               0               0               0  \n",
            "4               0               0               0  \n",
            "\n",
            "[5 rows x 231 columns]\n"
          ]
        }
      ],
      "source": [
        "df = df.drop('OwnerUserId', axis=1)\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdoG6m3JDNt-",
        "outputId": "dd495f00-8e2c-4c14-82ab-05de3cbd7cdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Params: {'hidden_units': 100, 'keep_prob': 0.9, 'learning_rate': 0.005, 'minibatch_size': 50, 'training_epoch': 20} - Best RMSE: 274.20690044242235\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "class RBM:\n",
        "    \"\"\"Restricted Boltzmann Machine\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        visible_units,\n",
        "        hidden_units=500,\n",
        "        keep_prob=0.7,\n",
        "        init_stdv=0.1,\n",
        "        learning_rate=0.004,\n",
        "        minibatch_size=100,\n",
        "        training_epoch=20,\n",
        "        display_epoch=10,\n",
        "        sampling_protocol=[50, 70, 80, 90, 100],\n",
        "        debug=False,\n",
        "        with_metrics=False,\n",
        "        seed=42,\n",
        "    ):\n",
        "        self.n_hidden = hidden_units\n",
        "        self.keep = keep_prob\n",
        "        self.stdv = init_stdv\n",
        "        self.learning_rate = learning_rate\n",
        "        self.minibatch = minibatch_size\n",
        "        self.epochs = training_epoch + 1\n",
        "        self.display_epoch = display_epoch\n",
        "        self.sampling_protocol = sampling_protocol\n",
        "        self.debug = debug\n",
        "        self.with_metrics = with_metrics\n",
        "        self.seed = seed\n",
        "        np.random.seed(self.seed)\n",
        "        tf.compat.v1.set_random_seed(self.seed)\n",
        "        self.n_visible = visible_units\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        self.generate_graph()\n",
        "        self.init_metrics()\n",
        "        self.init_gpu()\n",
        "        init_graph = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "        self.sess = tf.compat.v1.Session(config=self.config_gpu)\n",
        "        self.sess.run(init_graph)\n",
        "\n",
        "    def binomial_sampling(self, pr):\n",
        "        g = tf.convert_to_tensor(value=np.random.uniform(size=pr.shape[1]), dtype=tf.float32)\n",
        "        h_sampled = tf.nn.relu(tf.sign(pr - g))\n",
        "        return h_sampled\n",
        "\n",
        "    def free_energy(self, x):\n",
        "        bias = -tf.reduce_sum(input_tensor=tf.matmul(x, tf.transpose(a=self.bv)))\n",
        "        phi_x = tf.matmul(x, self.w) + self.bh\n",
        "        f = -tf.reduce_sum(input_tensor=tf.nn.softplus(phi_x))\n",
        "        F = bias + f\n",
        "        return F\n",
        "\n",
        "    def placeholder(self):\n",
        "        self.vu = tf.compat.v1.placeholder(shape=[None, self.n_visible], dtype=\"float32\")\n",
        "\n",
        "    def init_parameters(self):\n",
        "        with tf.compat.v1.variable_scope(\"Network_parameters\"):\n",
        "            self.w = tf.compat.v1.get_variable(\n",
        "                \"weight\",\n",
        "                [self.n_visible, self.n_hidden],\n",
        "                initializer=tf.compat.v1.random_normal_initializer(stddev=self.stdv, seed=self.seed),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "            self.bv = tf.compat.v1.get_variable(\n",
        "                \"v_bias\",\n",
        "                [1, self.n_visible],\n",
        "                initializer=tf.compat.v1.zeros_initializer(),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "            self.bh = tf.compat.v1.get_variable(\n",
        "                \"h_bias\",\n",
        "                [1, self.n_hidden],\n",
        "                initializer=tf.compat.v1.zeros_initializer(),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "\n",
        "    def sample_hidden_units(self, vv):\n",
        "        phi_v = tf.matmul(vv, self.w) + self.bh\n",
        "        phv = tf.nn.sigmoid(phi_v)\n",
        "        phv_reg = tf.nn.dropout(phv, 1 - (self.keep))\n",
        "        h_ = self.binomial_sampling(phv_reg)\n",
        "        return phv, h_\n",
        "\n",
        "    def sample_visible_units(self, h):\n",
        "        phi_h = tf.matmul(h, tf.transpose(a=self.w)) + self.bv\n",
        "        v_ = tf.nn.sigmoid(phi_h)\n",
        "        return v_\n",
        "\n",
        "    def gibbs_sampling(self):\n",
        "        self.v_k = self.v\n",
        "        for _ in range(self.k):\n",
        "            _, h_k = self.sample_hidden_units(self.v_k)\n",
        "            self.v_k = self.sample_visible_units(h_k)\n",
        "\n",
        "    def losses(self, vv):\n",
        "        with tf.compat.v1.variable_scope(\"losses\"):\n",
        "            obj = self.free_energy(vv) - self.free_energy(self.v_k)\n",
        "        return obj\n",
        "\n",
        "    def gibbs_protocol(self, i):\n",
        "        with tf.compat.v1.name_scope(\"gibbs_protocol\"):\n",
        "            epoch_percentage = (i / self.epochs) * 100\n",
        "            if epoch_percentage != 0:\n",
        "                if epoch_percentage >= self.sampling_protocol[self.l] and epoch_percentage <= self.sampling_protocol[self.l + 1]:\n",
        "                    self.k += 1\n",
        "                    self.l += 1\n",
        "                    self.gibbs_sampling()\n",
        "\n",
        "    def data_pipeline(self):\n",
        "        self.batch_size = tf.compat.v1.placeholder(tf.int64)\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices(self.vu)\n",
        "        self.dataset = self.dataset.shuffle(buffer_size=50, reshuffle_each_iteration=True, seed=self.seed)\n",
        "        self.dataset = self.dataset.batch(batch_size=self.batch_size).repeat()\n",
        "        self.iter = tf.compat.v1.data.make_initializable_iterator(self.dataset)\n",
        "        self.v = self.iter.get_next()\n",
        "\n",
        "    def init_metrics(self):\n",
        "        if self.with_metrics:\n",
        "            self.rmse = tf.sqrt(tf.compat.v1.losses.mean_squared_error(self.v, self.v_k, weights=tf.where(self.v > 0, 1, 0)))\n",
        "\n",
        "    def generate_graph(self):\n",
        "        log.info(\"Creating the computational graph\")\n",
        "        self.placeholder()\n",
        "        self.data_pipeline()\n",
        "        self.init_parameters()\n",
        "        log.info(\"Initialize Gibbs protocol\")\n",
        "        self.k = 1\n",
        "        self.l = 0\n",
        "        self.gibbs_sampling()\n",
        "        obj = self.losses(self.v)\n",
        "        rate = self.learning_rate / self.minibatch\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(learning_rate=rate).minimize(loss=obj)\n",
        "\n",
        "    def init_gpu(self):\n",
        "        self.config_gpu = tf.compat.v1.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
        "        self.config_gpu.gpu_options.allow_growth = True\n",
        "\n",
        "    def init_training_session(self, xtr):\n",
        "        self.sess.run(self.iter.initializer, feed_dict={self.vu: xtr, self.batch_size: self.minibatch})\n",
        "        self.sess.run(tf.compat.v1.tables_initializer())\n",
        "\n",
        "    def batch_training(self, num_minibatches):\n",
        "        epoch_tr_err = 0\n",
        "        for _ in range(num_minibatches):\n",
        "            if self.with_metrics:\n",
        "                _, batch_err = self.sess.run([self.opt, self.rmse])\n",
        "                epoch_tr_err += batch_err / num_minibatches\n",
        "            else:\n",
        "                _ = self.sess.run(self.opt)\n",
        "        return epoch_tr_err\n",
        "\n",
        "    def fit(self, xtr):\n",
        "        self.init_training_session(xtr)\n",
        "        for epoch in range(self.epochs):\n",
        "            self.gibbs_protocol(epoch)\n",
        "            tr_err = self.batch_training(num_minibatches=len(xtr) // self.minibatch)\n",
        "            if epoch % self.display_epoch == 0 or epoch == self.epochs - 1:\n",
        "                log.info(f\"Epoch {epoch}: Training error: {tr_err}\")\n",
        "\n",
        "    def predict(self, xtst):\n",
        "        self.sess.run(self.iter.initializer, feed_dict={self.vu: xtst, self.batch_size: xtst.shape[0]})\n",
        "        preds = self.sess.run(self.v_k)\n",
        "        return preds\n",
        "\n",
        "def hyperparameter_tuning(data, param_grid, test_size=0.2, random_state=42):\n",
        "    X_train, X_test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
        "    best_params = None\n",
        "    best_rmse = float('inf')\n",
        "    for params in ParameterGrid(param_grid):\n",
        "        model = RBM(visible_units=data.shape[1], **params)\n",
        "        model.fit(X_train)\n",
        "        predictions = model.predict(X_test)\n",
        "        log.info(f\"Shape of X_test: {X_test.shape}, Shape of predictions: {predictions.shape}\")\n",
        "        rmse = np.sqrt(mean_squared_error(X_test[X_test > 0], predictions[X_test > 0]))\n",
        "        log.info(f\"Params: {params} - RMSE: {rmse}\")\n",
        "        if rmse < best_rmse:\n",
        "            best_rmse = rmse\n",
        "            best_params = params\n",
        "    log.info(f\"Best Params: {best_params} - Best RMSE: {best_rmse}\")\n",
        "    return best_params, best_rmse\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    import pandas as pd\n",
        "\n",
        "    # Example data, replace with actual dataset\n",
        "    data = df\n",
        "\n",
        "    param_grid = {\n",
        "        'hidden_units': [50, 100],\n",
        "        'keep_prob': [0.7, 0.9],\n",
        "        'learning_rate': [0.01, 0.005],\n",
        "        'minibatch_size': [50, 100],\n",
        "        'training_epoch': [20, 50]\n",
        "    }\n",
        "    best_params, best_rmse = hyperparameter_tuning(data.values, param_grid)\n",
        "    print(f\"Best Params: {best_params} - Best RMSE: {best_rmse}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0fafa6-78e4-4e09-a66b-6b4dd28c970d",
        "id": "3ccom_92ELqe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hyperparameter Tuning:   0%|          | 0/4 [00:00<?, ?it/s]\n",
            "Training:   0%|          | 0/76 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   1%|▏         | 1/76 [00:18<23:23, 18.71s/it]\u001b[A\n",
            "Training:   3%|▎         | 2/76 [00:37<23:06, 18.74s/it]\u001b[A\n",
            "Training:   4%|▍         | 3/76 [00:55<22:28, 18.47s/it]\u001b[A\n",
            "Training:   5%|▌         | 4/76 [01:15<22:53, 19.08s/it]\u001b[A\n",
            "Training:   7%|▋         | 5/76 [01:33<21:53, 18.50s/it]\u001b[A\n",
            "Training:   8%|▊         | 6/76 [01:50<21:07, 18.11s/it]\u001b[A\n",
            "Training:   9%|▉         | 7/76 [02:07<20:36, 17.92s/it]\u001b[A\n",
            "Training:  11%|█         | 8/76 [02:28<21:05, 18.61s/it]\u001b[A\n",
            "Training:  12%|█▏        | 9/76 [02:46<20:46, 18.61s/it]\u001b[A\n",
            "Training:  13%|█▎        | 10/76 [03:04<20:05, 18.26s/it]\u001b[A\n",
            "Training:  14%|█▍        | 11/76 [03:21<19:31, 18.03s/it]\u001b[A\n",
            "Training:  16%|█▌        | 12/76 [03:39<19:05, 17.90s/it]\u001b[A\n",
            "Training:  17%|█▋        | 13/76 [03:57<18:45, 17.86s/it]\u001b[A\n",
            "Training:  18%|█▊        | 14/76 [04:16<18:58, 18.37s/it]\u001b[A\n",
            "Training:  20%|█▉        | 15/76 [04:33<18:21, 18.06s/it]\u001b[A\n",
            "Training:  21%|██        | 16/76 [04:51<17:51, 17.86s/it]\u001b[A\n",
            "Training:  22%|██▏       | 17/76 [05:08<17:27, 17.75s/it]\u001b[A\n",
            "Training:  24%|██▎       | 18/76 [05:26<17:06, 17.70s/it]\u001b[A\n",
            "Training:  25%|██▌       | 19/76 [05:45<17:18, 18.21s/it]\u001b[A\n",
            "Training:  26%|██▋       | 20/76 [06:04<17:01, 18.24s/it]\u001b[A\n",
            "Training:  28%|██▊       | 21/76 [06:21<16:31, 18.03s/it]\u001b[A\n",
            "Training:  29%|██▉       | 22/76 [06:39<16:03, 17.84s/it]\u001b[A\n",
            "Training:  30%|███       | 23/76 [06:56<15:38, 17.71s/it]\u001b[A\n",
            "Training:  32%|███▏      | 24/76 [07:15<15:42, 18.12s/it]\u001b[A\n",
            "Training:  33%|███▎      | 25/76 [07:33<15:27, 18.18s/it]\u001b[A\n",
            "Training:  34%|███▍      | 26/76 [07:51<14:57, 17.94s/it]\u001b[A\n",
            "Training:  36%|███▌      | 27/76 [08:08<14:31, 17.79s/it]\u001b[A\n",
            "Training:  37%|███▋      | 28/76 [08:26<14:09, 17.70s/it]\u001b[A\n",
            "Training:  38%|███▊      | 29/76 [08:43<13:49, 17.66s/it]\u001b[A\n",
            "Training:  39%|███▉      | 30/76 [09:05<14:23, 18.78s/it]\u001b[A\n",
            "Training:  41%|████      | 31/76 [09:24<14:09, 18.87s/it]\u001b[A\n",
            "Training:  42%|████▏     | 32/76 [09:41<13:30, 18.43s/it]\u001b[A\n",
            "Training:  43%|████▎     | 33/76 [09:58<12:58, 18.11s/it]\u001b[A\n",
            "Training:  45%|████▍     | 34/76 [10:16<12:35, 17.99s/it]\u001b[A\n",
            "Training:  46%|████▌     | 35/76 [10:36<12:35, 18.43s/it]\u001b[A\n",
            "Training:  47%|████▋     | 36/76 [10:53<12:07, 18.18s/it]\u001b[A\n",
            "Training:  49%|████▊     | 37/76 [11:11<11:42, 18.02s/it]\u001b[A\n",
            "Training:  50%|█████     | 38/76 [11:28<11:19, 17.88s/it]\u001b[A\n",
            "Training:  51%|█████▏    | 39/76 [11:46<10:57, 17.78s/it]\u001b[A\n",
            "Training:  53%|█████▎    | 40/76 [12:06<11:01, 18.37s/it]\u001b[A\n",
            "Training:  54%|█████▍    | 41/76 [12:24<10:38, 18.25s/it]\u001b[A\n",
            "Training:  55%|█████▌    | 42/76 [12:41<10:13, 18.04s/it]\u001b[A\n",
            "Training:  57%|█████▋    | 43/76 [12:59<09:49, 17.86s/it]\u001b[A\n",
            "Training:  58%|█████▊    | 44/76 [13:16<09:29, 17.78s/it]\u001b[A\n",
            "Training:  59%|█████▉    | 45/76 [13:36<09:29, 18.37s/it]\u001b[A\n",
            "Training:  61%|██████    | 46/76 [13:54<09:04, 18.13s/it]\u001b[A\n",
            "Training:  62%|██████▏   | 47/76 [14:11<08:37, 17.86s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 48/76 [14:28<08:14, 17.67s/it]\u001b[A\n",
            "Training:  64%|██████▍   | 49/76 [14:45<07:55, 17.59s/it]\u001b[A\n",
            "Training:  66%|██████▌   | 50/76 [15:05<07:49, 18.04s/it]\u001b[A\n",
            "Training:  67%|██████▋   | 51/76 [15:23<07:32, 18.10s/it]\u001b[A\n",
            "Training:  68%|██████▊   | 52/76 [15:40<07:08, 17.85s/it]\u001b[A\n",
            "Training:  70%|██████▉   | 53/76 [15:57<06:45, 17.65s/it]\u001b[A\n",
            "Training:  71%|███████   | 54/76 [16:14<06:25, 17.50s/it]\u001b[A\n",
            "Training:  72%|███████▏  | 55/76 [16:32<06:05, 17.42s/it]\u001b[A\n",
            "Training:  74%|███████▎  | 56/76 [16:50<05:56, 17.82s/it]\u001b[A\n",
            "Training:  75%|███████▌  | 57/76 [17:08<05:39, 17.85s/it]\u001b[A\n",
            "Training:  76%|███████▋  | 58/76 [17:26<05:19, 17.75s/it]\u001b[A\n",
            "Training:  78%|███████▊  | 59/76 [17:43<05:00, 17.66s/it]\u001b[A\n",
            "Training:  79%|███████▉  | 60/76 [18:01<04:41, 17.58s/it]\u001b[A\n",
            "Training:  80%|████████  | 61/76 [18:20<04:32, 18.15s/it]\u001b[A\n",
            "Training:  82%|████████▏ | 62/76 [18:39<04:17, 18.37s/it]\u001b[A\n",
            "Training:  83%|████████▎ | 63/76 [18:57<03:58, 18.33s/it]\u001b[A\n",
            "Training:  84%|████████▍ | 64/76 [19:16<03:40, 18.34s/it]\u001b[A\n",
            "Training:  86%|████████▌ | 65/76 [19:35<03:23, 18.54s/it]\u001b[A\n",
            "Training:  87%|████████▋ | 66/76 [19:53<03:06, 18.61s/it]\u001b[A\n",
            "Training:  88%|████████▊ | 67/76 [20:11<02:43, 18.21s/it]\u001b[A\n",
            "Training:  89%|████████▉ | 68/76 [20:28<02:23, 17.99s/it]\u001b[A\n",
            "Training:  91%|█████████ | 69/76 [20:46<02:04, 17.81s/it]\u001b[A\n",
            "Training:  92%|█████████▏| 70/76 [21:04<01:47, 17.91s/it]\u001b[A\n",
            "Training:  93%|█████████▎| 71/76 [21:23<01:31, 18.33s/it]\u001b[A\n",
            "Training:  95%|█████████▍| 72/76 [21:41<01:12, 18.11s/it]\u001b[A\n",
            "Training:  96%|█████████▌| 73/76 [21:58<00:53, 17.90s/it]\u001b[A\n",
            "Training:  97%|█████████▋| 74/76 [22:15<00:35, 17.71s/it]\u001b[A\n",
            "Training:  99%|█████████▊| 75/76 [22:32<00:17, 17.56s/it]\u001b[A\n",
            "Training: 100%|██████████| 76/76 [22:51<00:00, 18.05s/it]\n",
            "Hyperparameter Tuning:  25%|██▌       | 1/4 [24:16<1:12:49, 1456.54s/it]\n",
            "Training:   0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   1%|          | 1/101 [00:17<29:15, 17.55s/it]\u001b[A\n",
            "Training:   2%|▏         | 2/101 [00:34<28:48, 17.46s/it]\u001b[A\n",
            "Training:   3%|▎         | 3/101 [00:54<29:57, 18.34s/it]\u001b[A\n",
            "Training:   4%|▍         | 4/101 [01:13<29:57, 18.53s/it]\u001b[A\n",
            "Training:   5%|▍         | 5/101 [01:30<29:12, 18.26s/it]\u001b[A\n",
            "Training:   6%|▌         | 6/101 [01:48<28:30, 18.00s/it]\u001b[A\n",
            "Training:   7%|▋         | 7/101 [02:06<28:19, 18.08s/it]\u001b[A\n",
            "Training:   8%|▊         | 8/101 [02:26<28:39, 18.49s/it]\u001b[A\n",
            "Training:   9%|▉         | 9/101 [02:43<27:50, 18.16s/it]\u001b[A\n",
            "Training:  10%|▉         | 10/101 [03:00<27:11, 17.93s/it]\u001b[A\n",
            "Training:  11%|█         | 11/101 [03:18<26:37, 17.75s/it]\u001b[A\n",
            "Training:  12%|█▏        | 12/101 [03:35<26:07, 17.62s/it]\u001b[A\n",
            "Training:  13%|█▎        | 13/101 [03:54<26:35, 18.13s/it]\u001b[A\n",
            "Training:  14%|█▍        | 14/101 [04:12<26:08, 18.03s/it]\u001b[A\n",
            "Training:  15%|█▍        | 15/101 [04:30<25:38, 17.89s/it]\u001b[A\n",
            "Training:  16%|█▌        | 16/101 [04:47<25:11, 17.79s/it]\u001b[A\n",
            "Training:  17%|█▋        | 17/101 [05:05<24:55, 17.81s/it]\u001b[A\n",
            "Training:  18%|█▊        | 18/101 [05:25<25:36, 18.51s/it]\u001b[A\n",
            "Training:  19%|█▉        | 19/101 [05:43<24:57, 18.26s/it]\u001b[A\n",
            "Training:  20%|█▉        | 20/101 [06:01<24:26, 18.11s/it]\u001b[A\n",
            "Training:  21%|██        | 21/101 [06:18<23:55, 17.95s/it]\u001b[A\n",
            "Training:  22%|██▏       | 22/101 [06:37<23:44, 18.03s/it]\u001b[A\n",
            "Training:  23%|██▎       | 23/101 [06:56<24:06, 18.55s/it]\u001b[A\n",
            "Training:  24%|██▍       | 24/101 [07:14<23:37, 18.41s/it]\u001b[A\n",
            "Training:  25%|██▍       | 25/101 [07:33<23:13, 18.34s/it]\u001b[A\n",
            "Training:  26%|██▌       | 26/101 [07:51<22:51, 18.29s/it]\u001b[A\n",
            "Training:  27%|██▋       | 27/101 [08:11<23:22, 18.96s/it]\u001b[A\n",
            "Training:  28%|██▊       | 28/101 [08:29<22:44, 18.69s/it]\u001b[A\n",
            "Training:  29%|██▊       | 29/101 [08:47<22:04, 18.39s/it]\u001b[A\n",
            "Training:  30%|██▉       | 30/101 [09:05<21:30, 18.18s/it]\u001b[A\n",
            "Training:  31%|███       | 31/101 [09:23<21:23, 18.34s/it]\u001b[A\n",
            "Training:  32%|███▏      | 32/101 [09:42<21:20, 18.55s/it]\u001b[A\n",
            "Training:  33%|███▎      | 33/101 [10:00<20:42, 18.27s/it]\u001b[A\n",
            "Training:  34%|███▎      | 34/101 [10:21<21:15, 19.03s/it]\u001b[A\n",
            "Training:  35%|███▍      | 35/101 [10:39<20:30, 18.64s/it]\u001b[A\n",
            "Training:  36%|███▌      | 36/101 [10:59<20:38, 19.05s/it]\u001b[A\n",
            "Training:  37%|███▋      | 37/101 [11:17<20:10, 18.92s/it]\u001b[A\n",
            "Training:  38%|███▊      | 38/101 [11:35<19:35, 18.65s/it]\u001b[A\n",
            "Training:  39%|███▊      | 39/101 [11:54<19:22, 18.74s/it]\u001b[A\n",
            "Training:  40%|███▉      | 40/101 [12:15<19:47, 19.47s/it]\u001b[A\n",
            "Training:  41%|████      | 41/101 [12:36<19:43, 19.72s/it]\u001b[A\n",
            "Training:  42%|████▏     | 42/101 [12:55<19:18, 19.63s/it]\u001b[A\n",
            "Training:  43%|████▎     | 43/101 [13:19<20:08, 20.83s/it]\u001b[A\n",
            "Training:  44%|████▎     | 44/101 [13:38<19:29, 20.51s/it]\u001b[A\n",
            "Training:  45%|████▍     | 45/101 [14:04<20:35, 22.05s/it]\u001b[A\n",
            "Training:  46%|████▌     | 46/101 [14:25<19:49, 21.62s/it]\u001b[A\n",
            "Training:  47%|████▋     | 47/101 [14:45<19:01, 21.15s/it]\u001b[A\n",
            "Training:  48%|████▊     | 48/101 [15:06<18:43, 21.19s/it]\u001b[A\n",
            "Training:  49%|████▊     | 49/101 [15:25<17:51, 20.61s/it]\u001b[A\n",
            "Training:  50%|████▉     | 50/101 [15:46<17:27, 20.55s/it]\u001b[A\n",
            "Training:  50%|█████     | 51/101 [16:06<16:59, 20.39s/it]\u001b[A\n",
            "Training:  51%|█████▏    | 52/101 [16:25<16:21, 20.03s/it]\u001b[A\n",
            "Training:  52%|█████▏    | 53/101 [16:44<15:51, 19.82s/it]\u001b[A\n",
            "Training:  53%|█████▎    | 54/101 [17:05<15:48, 20.19s/it]\u001b[A\n",
            "Training:  54%|█████▍    | 55/101 [17:24<15:14, 19.88s/it]\u001b[A\n",
            "Training:  55%|█████▌    | 56/101 [17:43<14:32, 19.38s/it]\u001b[A\n",
            "Training:  56%|█████▋    | 57/101 [18:04<14:34, 19.88s/it]\u001b[A\n",
            "Training:  57%|█████▋    | 58/101 [18:22<13:56, 19.45s/it]\u001b[A\n",
            "Training:  58%|█████▊    | 59/101 [18:41<13:33, 19.38s/it]\u001b[A\n",
            "Training:  59%|█████▉    | 60/101 [19:03<13:42, 20.06s/it]\u001b[A\n",
            "Training:  60%|██████    | 61/101 [19:22<13:07, 19.68s/it]\u001b[A\n",
            "Training:  61%|██████▏   | 62/101 [19:41<12:42, 19.55s/it]\u001b[A\n",
            "Training:  62%|██████▏   | 63/101 [20:02<12:41, 20.04s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 64/101 [20:21<12:08, 19.70s/it]\u001b[A\n",
            "Training:  64%|██████▍   | 65/101 [20:41<11:45, 19.59s/it]\u001b[A\n",
            "Training:  65%|██████▌   | 66/101 [21:02<11:45, 20.15s/it]\u001b[A\n",
            "Training:  66%|██████▋   | 67/101 [21:21<11:14, 19.84s/it]\u001b[A\n",
            "Training:  67%|██████▋   | 68/101 [21:41<10:56, 19.90s/it]\u001b[A\n",
            "Training:  68%|██████▊   | 69/101 [22:03<10:56, 20.52s/it]\u001b[A\n",
            "Training:  69%|██████▉   | 70/101 [22:23<10:30, 20.35s/it]\u001b[A\n",
            "Training:  70%|███████   | 71/101 [22:42<10:01, 20.05s/it]\u001b[A\n",
            "Training:  71%|███████▏  | 72/101 [23:04<09:53, 20.46s/it]\u001b[A\n",
            "Training:  72%|███████▏  | 73/101 [23:22<09:17, 19.91s/it]\u001b[A\n",
            "Training:  73%|███████▎  | 74/101 [23:41<08:49, 19.60s/it]\u001b[A\n",
            "Training:  74%|███████▍  | 75/101 [24:02<08:36, 19.86s/it]\u001b[A\n",
            "Training:  75%|███████▌  | 76/101 [24:20<08:06, 19.48s/it]\u001b[A\n",
            "Training:  76%|███████▌  | 77/101 [24:39<07:42, 19.26s/it]\u001b[A\n",
            "Training:  77%|███████▋  | 78/101 [24:58<07:18, 19.05s/it]\u001b[A\n",
            "Training:  78%|███████▊  | 79/101 [25:18<07:07, 19.41s/it]\u001b[A\n",
            "Training:  79%|███████▉  | 80/101 [25:36<06:39, 19.02s/it]\u001b[A\n",
            "Training:  80%|████████  | 81/101 [25:54<06:15, 18.78s/it]\u001b[A\n",
            "Training:  81%|████████  | 82/101 [26:13<05:58, 18.85s/it]\u001b[A\n",
            "Training:  82%|████████▏ | 83/101 [26:33<05:45, 19.18s/it]\u001b[A\n",
            "Training:  83%|████████▎ | 84/101 [26:52<05:21, 18.92s/it]\u001b[A\n",
            "Training:  84%|████████▍ | 85/101 [27:10<04:59, 18.70s/it]\u001b[A\n",
            "Training:  85%|████████▌ | 86/101 [27:28<04:40, 18.69s/it]\u001b[A\n",
            "Training:  86%|████████▌ | 87/101 [27:48<04:26, 19.05s/it]\u001b[A\n",
            "Training:  87%|████████▋ | 88/101 [28:06<04:02, 18.62s/it]\u001b[A\n",
            "Training:  88%|████████▊ | 89/101 [28:24<03:40, 18.35s/it]\u001b[A\n",
            "Training:  89%|████████▉ | 90/101 [28:41<03:20, 18.21s/it]\u001b[A\n",
            "Training:  90%|█████████ | 91/101 [29:02<03:09, 18.95s/it]\u001b[A\n",
            "Training:  91%|█████████ | 92/101 [29:20<02:48, 18.72s/it]\u001b[A\n",
            "Training:  92%|█████████▏| 93/101 [29:39<02:29, 18.67s/it]\u001b[A\n",
            "Training:  93%|█████████▎| 94/101 [29:57<02:09, 18.51s/it]\u001b[A\n",
            "Training:  94%|█████████▍| 95/101 [30:17<01:54, 19.09s/it]\u001b[A\n",
            "Training:  95%|█████████▌| 96/101 [30:36<01:34, 18.82s/it]\u001b[A\n",
            "Training:  96%|█████████▌| 97/101 [30:54<01:14, 18.72s/it]\u001b[A\n",
            "Training:  97%|█████████▋| 98/101 [31:12<00:55, 18.45s/it]\u001b[A\n",
            "Training:  98%|█████████▊| 99/101 [31:32<00:37, 18.87s/it]\u001b[A\n",
            "Training:  99%|█████████▉| 100/101 [31:50<00:18, 18.61s/it]\u001b[A\n",
            "Training: 100%|██████████| 101/101 [32:08<00:00, 19.09s/it]\n",
            "Hyperparameter Tuning:  50%|█████     | 2/4 [58:02<59:42, 1791.43s/it]  \n",
            "Training:   0%|          | 0/76 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   1%|▏         | 1/76 [00:17<22:16, 17.82s/it]\u001b[A\n",
            "Training:   3%|▎         | 2/76 [00:36<22:21, 18.12s/it]\u001b[A\n",
            "Training:   4%|▍         | 3/76 [00:54<21:53, 18.00s/it]\u001b[A\n",
            "Training:   5%|▌         | 4/76 [01:13<22:10, 18.48s/it]\u001b[A\n",
            "Training:   7%|▋         | 5/76 [01:30<21:24, 18.09s/it]\u001b[A\n",
            "Training:   8%|▊         | 6/76 [01:47<20:37, 17.68s/it]\u001b[A\n",
            "Training:   9%|▉         | 7/76 [02:04<19:54, 17.31s/it]\u001b[A\n",
            "Training:  11%|█         | 8/76 [02:20<19:27, 17.17s/it]\u001b[A\n",
            "Training:  12%|█▏        | 9/76 [02:37<18:59, 17.01s/it]\u001b[A\n",
            "Training:  13%|█▎        | 10/76 [02:54<18:33, 16.88s/it]\u001b[A\n",
            "Training:  14%|█▍        | 11/76 [03:12<18:44, 17.31s/it]\u001b[A\n",
            "Training:  16%|█▌        | 12/76 [03:29<18:29, 17.34s/it]\u001b[A\n",
            "Training:  17%|█▋        | 13/76 [03:46<17:58, 17.13s/it]\u001b[A\n",
            "Training:  18%|█▊        | 14/76 [04:03<17:37, 17.05s/it]\u001b[A\n",
            "Training:  20%|█▉        | 15/76 [04:19<17:09, 16.87s/it]\u001b[A\n",
            "Training:  21%|██        | 16/76 [04:36<16:55, 16.92s/it]\u001b[A\n",
            "Training:  22%|██▏       | 17/76 [04:53<16:41, 16.98s/it]\u001b[A\n",
            "Training:  24%|██▎       | 18/76 [05:12<16:55, 17.52s/it]\u001b[A\n",
            "Training:  25%|██▌       | 19/76 [05:30<16:41, 17.58s/it]\u001b[A\n",
            "Training:  26%|██▋       | 20/76 [05:47<16:15, 17.42s/it]\u001b[A\n",
            "Training:  28%|██▊       | 21/76 [06:04<15:51, 17.30s/it]\u001b[A\n",
            "Training:  29%|██▉       | 22/76 [06:21<15:32, 17.27s/it]\u001b[A\n",
            "Training:  30%|███       | 23/76 [06:38<15:08, 17.15s/it]\u001b[A\n",
            "Training:  32%|███▏      | 24/76 [06:55<14:51, 17.15s/it]\u001b[A\n",
            "Training:  33%|███▎      | 25/76 [07:13<14:49, 17.44s/it]\u001b[A\n",
            "Training:  34%|███▍      | 26/76 [07:31<14:36, 17.52s/it]\u001b[A\n",
            "Training:  36%|███▌      | 27/76 [07:48<14:13, 17.41s/it]\u001b[A\n",
            "Training:  37%|███▋      | 28/76 [08:05<13:51, 17.32s/it]\u001b[A\n",
            "Training:  38%|███▊      | 29/76 [08:22<13:24, 17.11s/it]\u001b[A\n",
            "Training:  39%|███▉      | 30/76 [08:39<13:02, 17.01s/it]\u001b[A\n",
            "Training:  41%|████      | 31/76 [08:56<12:47, 17.06s/it]\u001b[A\n",
            "Training:  42%|████▏     | 32/76 [09:14<12:48, 17.47s/it]\u001b[A\n",
            "Training:  43%|████▎     | 33/76 [09:32<12:28, 17.40s/it]\u001b[A\n",
            "Training:  45%|████▍     | 34/76 [09:49<12:05, 17.27s/it]\u001b[A\n",
            "Training:  46%|████▌     | 35/76 [10:06<11:44, 17.19s/it]\u001b[A\n",
            "Training:  47%|████▋     | 36/76 [10:23<11:25, 17.15s/it]\u001b[A\n",
            "Training:  49%|████▊     | 37/76 [10:39<11:05, 17.06s/it]\u001b[A\n",
            "Training:  50%|█████     | 38/76 [10:57<10:55, 17.26s/it]\u001b[A\n",
            "Training:  51%|█████▏    | 39/76 [11:16<10:50, 17.57s/it]\u001b[A\n",
            "Training:  53%|█████▎    | 40/76 [11:32<10:25, 17.38s/it]\u001b[A\n",
            "Training:  54%|█████▍    | 41/76 [11:50<10:05, 17.29s/it]\u001b[A\n",
            "Training:  55%|█████▌    | 42/76 [12:07<09:47, 17.29s/it]\u001b[A\n",
            "Training:  57%|█████▋    | 43/76 [12:24<09:25, 17.14s/it]\u001b[A\n",
            "Training:  58%|█████▊    | 44/76 [12:40<09:04, 17.00s/it]\u001b[A\n",
            "Training:  59%|█████▉    | 45/76 [12:58<08:49, 17.10s/it]\u001b[A\n",
            "Training:  61%|██████    | 46/76 [13:16<08:43, 17.46s/it]\u001b[A\n",
            "Training:  62%|██████▏   | 47/76 [13:32<08:16, 17.12s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 48/76 [13:49<07:55, 16.97s/it]\u001b[A\n",
            "Training:  64%|██████▍   | 49/76 [14:06<07:36, 16.91s/it]\u001b[A\n",
            "Training:  66%|██████▌   | 50/76 [14:22<07:17, 16.83s/it]\u001b[A\n",
            "Training:  67%|██████▋   | 51/76 [14:39<06:59, 16.78s/it]\u001b[A\n",
            "Training:  68%|██████▊   | 52/76 [14:55<06:39, 16.65s/it]\u001b[A\n",
            "Training:  70%|██████▉   | 53/76 [15:13<06:27, 16.85s/it]\u001b[A\n",
            "Training:  71%|███████   | 54/76 [15:30<06:17, 17.14s/it]\u001b[A\n",
            "Training:  72%|███████▏  | 55/76 [15:47<05:57, 17.03s/it]\u001b[A\n",
            "Training:  74%|███████▎  | 56/76 [16:04<05:40, 17.00s/it]\u001b[A\n",
            "Training:  75%|███████▌  | 57/76 [16:21<05:21, 16.91s/it]\u001b[A\n",
            "Training:  76%|███████▋  | 58/76 [16:37<05:02, 16.80s/it]\u001b[A\n",
            "Training:  78%|███████▊  | 59/76 [16:54<04:44, 16.72s/it]\u001b[A\n",
            "Training:  79%|███████▉  | 60/76 [17:10<04:25, 16.60s/it]\u001b[A\n",
            "Training:  80%|████████  | 61/76 [17:27<04:09, 16.63s/it]\u001b[A\n",
            "Training:  82%|████████▏ | 62/76 [17:45<03:58, 17.01s/it]\u001b[A\n",
            "Training:  83%|████████▎ | 63/76 [18:02<03:40, 16.99s/it]\u001b[A\n",
            "Training:  84%|████████▍ | 64/76 [18:18<03:21, 16.81s/it]\u001b[A\n",
            "Training:  86%|████████▌ | 65/76 [18:34<03:02, 16.58s/it]\u001b[A\n",
            "Training:  87%|████████▋ | 66/76 [18:50<02:44, 16.47s/it]\u001b[A\n",
            "Training:  88%|████████▊ | 67/76 [19:07<02:27, 16.44s/it]\u001b[A\n",
            "Training:  89%|████████▉ | 68/76 [19:23<02:11, 16.48s/it]\u001b[A\n",
            "Training:  91%|█████████ | 69/76 [19:40<01:55, 16.56s/it]\u001b[A\n",
            "Training:  92%|█████████▏| 70/76 [19:57<01:40, 16.79s/it]\u001b[A\n",
            "Training:  93%|█████████▎| 71/76 [20:16<01:26, 17.24s/it]\u001b[A\n",
            "Training:  95%|█████████▍| 72/76 [20:33<01:09, 17.29s/it]\u001b[A\n",
            "Training:  96%|█████████▌| 73/76 [20:50<00:51, 17.11s/it]\u001b[A\n",
            "Training:  97%|█████████▋| 74/76 [21:06<00:33, 16.98s/it]\u001b[A\n",
            "Training:  99%|█████████▊| 75/76 [21:23<00:16, 16.78s/it]\u001b[A\n",
            "Training: 100%|██████████| 76/76 [21:39<00:00, 17.10s/it]\n",
            "Hyperparameter Tuning:  75%|███████▌  | 3/4 [1:21:19<26:51, 1611.15s/it]\n",
            "Training:   0%|          | 0/101 [00:00<?, ?it/s]\u001b[A\n",
            "Training:   1%|          | 1/101 [00:16<27:52, 16.72s/it]\u001b[A\n",
            "Training:   2%|▏         | 2/101 [00:34<28:27, 17.25s/it]\u001b[A\n",
            "Training:   3%|▎         | 3/101 [00:52<28:48, 17.63s/it]\u001b[A\n",
            "Training:   4%|▍         | 4/101 [01:09<27:55, 17.27s/it]\u001b[A\n",
            "Training:   5%|▍         | 5/101 [01:25<27:06, 16.94s/it]\u001b[A\n",
            "Training:   6%|▌         | 6/101 [01:42<26:38, 16.83s/it]\u001b[A\n",
            "Training:   7%|▋         | 7/101 [01:58<26:00, 16.60s/it]\u001b[A\n",
            "Training:   8%|▊         | 8/101 [02:14<25:43, 16.59s/it]\u001b[A\n",
            "Training:   9%|▉         | 9/101 [02:31<25:24, 16.57s/it]\u001b[A\n",
            "Training:  10%|▉         | 10/101 [02:48<25:15, 16.65s/it]\u001b[A\n",
            "Training:  11%|█         | 11/101 [03:06<25:32, 17.02s/it]\u001b[A\n",
            "Training:  12%|█▏        | 12/101 [03:23<25:19, 17.08s/it]\u001b[A\n",
            "Training:  13%|█▎        | 13/101 [03:39<24:45, 16.88s/it]\u001b[A\n",
            "Training:  14%|█▍        | 14/101 [03:56<24:26, 16.85s/it]\u001b[A\n",
            "Training:  15%|█▍        | 15/101 [04:13<24:09, 16.86s/it]\u001b[A\n",
            "Training:  16%|█▌        | 16/101 [04:30<23:58, 16.92s/it]\u001b[A\n",
            "Training:  17%|█▋        | 17/101 [04:47<23:54, 17.08s/it]\u001b[A\n",
            "Training:  18%|█▊        | 18/101 [05:06<24:23, 17.63s/it]\u001b[A\n",
            "Training:  19%|█▉        | 19/101 [05:24<24:03, 17.60s/it]\u001b[A\n",
            "Training:  20%|█▉        | 20/101 [05:41<23:35, 17.48s/it]\u001b[A\n",
            "Training:  21%|██        | 21/101 [05:58<22:56, 17.21s/it]\u001b[A\n",
            "Training:  22%|██▏       | 22/101 [06:14<22:23, 17.00s/it]\u001b[A\n",
            "Training:  23%|██▎       | 23/101 [06:31<21:58, 16.91s/it]\u001b[A\n",
            "Training:  24%|██▍       | 24/101 [06:47<21:33, 16.80s/it]\u001b[A\n",
            "Training:  25%|██▍       | 25/101 [07:06<21:50, 17.24s/it]\u001b[A\n",
            "Training:  26%|██▌       | 26/101 [07:23<21:44, 17.40s/it]\u001b[A\n",
            "Training:  27%|██▋       | 27/101 [07:40<21:18, 17.27s/it]\u001b[A\n",
            "Training:  28%|██▊       | 28/101 [07:57<20:56, 17.22s/it]\u001b[A\n",
            "Training:  29%|██▊       | 29/101 [08:14<20:31, 17.10s/it]\u001b[A\n",
            "Training:  30%|██▉       | 30/101 [08:31<20:05, 16.99s/it]\u001b[A\n",
            "Training:  31%|███       | 31/101 [08:48<19:44, 16.93s/it]\u001b[A\n",
            "Training:  32%|███▏      | 32/101 [09:05<19:39, 17.09s/it]\u001b[A\n",
            "Training:  33%|███▎      | 33/101 [09:23<19:33, 17.26s/it]\u001b[A\n",
            "Training:  34%|███▎      | 34/101 [09:40<19:13, 17.22s/it]\u001b[A\n",
            "Training:  35%|███▍      | 35/101 [09:57<18:46, 17.06s/it]\u001b[A\n",
            "Training:  36%|███▌      | 36/101 [10:14<18:25, 17.01s/it]\u001b[A\n",
            "Training:  37%|███▋      | 37/101 [10:30<18:01, 16.90s/it]\u001b[A\n",
            "Training:  38%|███▊      | 38/101 [10:47<17:38, 16.81s/it]\u001b[A\n",
            "Training:  39%|███▊      | 39/101 [11:04<17:21, 16.81s/it]\u001b[A\n",
            "Training:  40%|███▉      | 40/101 [11:21<17:19, 17.04s/it]\u001b[A\n",
            "Training:  41%|████      | 41/101 [11:39<17:22, 17.37s/it]\u001b[A\n",
            "Training:  42%|████▏     | 42/101 [11:56<16:55, 17.20s/it]\u001b[A\n",
            "Training:  43%|████▎     | 43/101 [12:13<16:35, 17.16s/it]\u001b[A\n",
            "Training:  44%|████▎     | 44/101 [12:30<16:12, 17.06s/it]\u001b[A\n",
            "Training:  45%|████▍     | 45/101 [12:47<15:50, 16.98s/it]\u001b[A\n",
            "Training:  46%|████▌     | 46/101 [13:04<15:29, 16.91s/it]\u001b[A\n",
            "Training:  47%|████▋     | 47/101 [13:21<15:14, 16.93s/it]\u001b[A\n",
            "Training:  48%|████▊     | 48/101 [13:38<15:10, 17.19s/it]\u001b[A\n",
            "Training:  49%|████▊     | 49/101 [13:56<15:03, 17.37s/it]\u001b[A\n",
            "Training:  50%|████▉     | 50/101 [14:13<14:37, 17.22s/it]\u001b[A\n",
            "Training:  50%|█████     | 51/101 [14:30<14:18, 17.18s/it]\u001b[A\n",
            "Training:  51%|█████▏    | 52/101 [14:47<13:58, 17.12s/it]\u001b[A\n",
            "Training:  52%|█████▏    | 53/101 [15:04<13:39, 17.08s/it]\u001b[A\n",
            "Training:  53%|█████▎    | 54/101 [15:21<13:20, 17.02s/it]\u001b[A\n",
            "Training:  54%|█████▍    | 55/101 [15:39<13:14, 17.27s/it]\u001b[A\n",
            "Training:  55%|█████▌    | 56/101 [15:57<13:04, 17.42s/it]\u001b[A\n",
            "Training:  56%|█████▋    | 57/101 [16:14<12:41, 17.32s/it]\u001b[A\n",
            "Training:  57%|█████▋    | 58/101 [16:31<12:21, 17.25s/it]\u001b[A\n",
            "Training:  58%|█████▊    | 59/101 [16:48<12:01, 17.18s/it]\u001b[A\n",
            "Training:  59%|█████▉    | 60/101 [17:05<11:42, 17.12s/it]\u001b[A\n",
            "Training:  60%|██████    | 61/101 [17:22<11:21, 17.05s/it]\u001b[A\n",
            "Training:  61%|██████▏   | 62/101 [17:40<11:20, 17.45s/it]\u001b[A\n",
            "Training:  62%|██████▏   | 63/101 [17:58<11:07, 17.56s/it]\u001b[A\n",
            "Training:  63%|██████▎   | 64/101 [18:15<10:40, 17.30s/it]\u001b[A\n",
            "Training:  64%|██████▍   | 65/101 [18:31<10:18, 17.17s/it]\u001b[A\n",
            "Training:  65%|██████▌   | 66/101 [18:48<09:55, 17.02s/it]\u001b[A\n",
            "Training:  66%|██████▋   | 67/101 [19:04<09:31, 16.82s/it]\u001b[A\n",
            "Training:  67%|██████▋   | 68/101 [19:21<09:11, 16.71s/it]\u001b[A\n",
            "Training:  68%|██████▊   | 69/101 [19:38<09:01, 16.93s/it]\u001b[A\n",
            "Training:  69%|██████▉   | 70/101 [19:56<08:52, 17.17s/it]\u001b[A\n",
            "Training:  70%|███████   | 71/101 [20:13<08:34, 17.16s/it]\u001b[A\n",
            "Training:  71%|███████▏  | 72/101 [20:30<08:13, 17.02s/it]\u001b[A\n",
            "Training:  72%|███████▏  | 73/101 [20:46<07:52, 16.87s/it]\u001b[A\n",
            "Training:  73%|███████▎  | 74/101 [21:03<07:35, 16.88s/it]\u001b[A\n",
            "Training:  74%|███████▍  | 75/101 [21:20<07:17, 16.84s/it]\u001b[A\n",
            "Training:  75%|███████▌  | 76/101 [21:37<07:00, 16.81s/it]\u001b[A\n",
            "Training:  76%|███████▌  | 77/101 [21:54<06:48, 17.04s/it]\u001b[A\n",
            "Training:  77%|███████▋  | 78/101 [22:12<06:38, 17.33s/it]\u001b[A\n",
            "Training:  78%|███████▊  | 79/101 [22:29<06:17, 17.17s/it]\u001b[A\n",
            "Training:  79%|███████▉  | 80/101 [22:46<05:56, 16.98s/it]\u001b[A\n",
            "Training:  80%|████████  | 81/101 [23:02<05:34, 16.75s/it]\u001b[A\n",
            "Training:  81%|████████  | 82/101 [23:19<05:17, 16.70s/it]\u001b[A\n",
            "Training:  82%|████████▏ | 83/101 [23:35<04:58, 16.56s/it]\u001b[A\n",
            "Training:  83%|████████▎ | 84/101 [23:51<04:40, 16.49s/it]\u001b[A\n",
            "Training:  84%|████████▍ | 85/101 [24:09<04:28, 16.81s/it]\u001b[A\n",
            "Training:  85%|████████▌ | 86/101 [24:26<04:16, 17.09s/it]\u001b[A\n",
            "Training:  86%|████████▌ | 87/101 [24:43<03:59, 17.09s/it]\u001b[A\n",
            "Training:  87%|████████▋ | 88/101 [25:00<03:39, 16.90s/it]\u001b[A\n",
            "Training:  88%|████████▊ | 89/101 [25:17<03:21, 16.82s/it]\u001b[A\n",
            "Training:  89%|████████▉ | 90/101 [25:33<03:04, 16.78s/it]\u001b[A\n",
            "Training:  90%|█████████ | 91/101 [25:50<02:46, 16.69s/it]\u001b[A\n",
            "Training:  91%|█████████ | 92/101 [26:06<02:30, 16.67s/it]\u001b[A\n",
            "Training:  92%|█████████▏| 93/101 [26:23<02:13, 16.65s/it]\u001b[A\n",
            "Training:  93%|█████████▎| 94/101 [26:41<01:59, 17.01s/it]\u001b[A\n",
            "Training:  94%|█████████▍| 95/101 [26:59<01:43, 17.25s/it]\u001b[A\n",
            "Training:  95%|█████████▌| 96/101 [27:15<01:25, 17.10s/it]\u001b[A\n",
            "Training:  96%|█████████▌| 97/101 [27:32<01:08, 17.02s/it]\u001b[A\n",
            "Training:  97%|█████████▋| 98/101 [27:49<00:51, 17.02s/it]\u001b[A\n",
            "Training:  98%|█████████▊| 99/101 [28:07<00:34, 17.19s/it]\u001b[A\n",
            "Training:  99%|█████████▉| 100/101 [28:23<00:17, 17.02s/it]\u001b[A\n",
            "Training: 100%|██████████| 101/101 [28:41<00:00, 17.04s/it]\n",
            "Hyperparameter Tuning: 100%|██████████| 4/4 [1:51:33<00:00, 1673.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'hidden_units': 100, 'keep_prob': 0.9, 'learning_rate': 0.005, 'minibatch_size': 100, 'training_epoch': 75}\n",
            "Best NDCG@5: 0.4473765333411958\n",
            "Best Recall@5: 0.780980492009718\n",
            "Best Precision@5: 0.1597259442787076\n",
            "Best MAP@5: 0.33025482329398426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 76/76 [27:44<00:00, 21.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 recommendations for user 0: [215 221  89 194 161]\n",
            "Recommended item IDs: ['sorting', 'list', 'data-structures', 'recursion', 'memory-management']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import logging\n",
        "from sklearn.model_selection import train_test_split, ParameterGrid\n",
        "from sklearn.metrics import ndcg_score\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "class RBM:\n",
        "    \"\"\"Restricted Boltzmann Machine for Recommendation System\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        visible_units,\n",
        "        hidden_units=500,\n",
        "        keep_prob=0.7,\n",
        "        init_stdv=0.1,\n",
        "        learning_rate=0.004,\n",
        "        minibatch_size=100,\n",
        "        training_epoch=20,\n",
        "        display_epoch=10,\n",
        "        sampling_protocol=[50, 70, 80, 90, 100],\n",
        "        debug=False,\n",
        "        with_metrics=True,\n",
        "        seed=42,\n",
        "    ):\n",
        "        self.n_hidden = hidden_units\n",
        "        self.keep = keep_prob\n",
        "        self.stdv = init_stdv\n",
        "        self.learning_rate = learning_rate\n",
        "        self.minibatch = minibatch_size\n",
        "        self.epochs = training_epoch + 1\n",
        "        self.display_epoch = display_epoch\n",
        "        self.sampling_protocol = sampling_protocol\n",
        "        self.debug = debug\n",
        "        self.with_metrics = with_metrics\n",
        "        self.seed = seed\n",
        "        np.random.seed(self.seed)\n",
        "        tf.compat.v1.set_random_seed(self.seed)\n",
        "        self.n_visible = visible_units\n",
        "\n",
        "        tf.compat.v1.reset_default_graph()\n",
        "\n",
        "        self.generate_graph()\n",
        "        self.init_metrics()\n",
        "        self.init_gpu()\n",
        "        init_graph = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "        self.sess = tf.compat.v1.Session(config=self.config_gpu)\n",
        "        self.sess.run(init_graph)\n",
        "\n",
        "    def binomial_sampling(self, pr):\n",
        "        g = tf.convert_to_tensor(value=np.random.uniform(size=pr.shape[1]), dtype=tf.float32)\n",
        "        h_sampled = tf.nn.relu(tf.sign(pr - g))\n",
        "        return h_sampled\n",
        "\n",
        "    def free_energy(self, x):\n",
        "        bias = -tf.reduce_sum(input_tensor=tf.matmul(x, tf.transpose(a=self.bv)))\n",
        "        phi_x = tf.matmul(x, self.w) + self.bh\n",
        "        f = -tf.reduce_sum(input_tensor=tf.nn.softplus(phi_x))\n",
        "        F = bias + f\n",
        "        return F\n",
        "\n",
        "    def placeholder(self):\n",
        "        self.vu = tf.compat.v1.placeholder(shape=[None, self.n_visible], dtype=\"float32\")\n",
        "\n",
        "    def init_parameters(self):\n",
        "        with tf.compat.v1.variable_scope(\"Network_parameters\"):\n",
        "            self.w = tf.compat.v1.get_variable(\n",
        "                \"weight\",\n",
        "                [self.n_visible, self.n_hidden],\n",
        "                initializer=tf.compat.v1.random_normal_initializer(stddev=self.stdv, seed=self.seed),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "            self.bv = tf.compat.v1.get_variable(\n",
        "                \"v_bias\",\n",
        "                [1, self.n_visible],\n",
        "                initializer=tf.compat.v1.zeros_initializer(),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "            self.bh = tf.compat.v1.get_variable(\n",
        "                \"h_bias\",\n",
        "                [1, self.n_hidden],\n",
        "                initializer=tf.compat.v1.zeros_initializer(),\n",
        "                dtype=\"float32\",\n",
        "            )\n",
        "\n",
        "    def sample_hidden_units(self, vv):\n",
        "        phi_v = tf.matmul(vv, self.w) + self.bh\n",
        "        phv = tf.nn.sigmoid(phi_v)\n",
        "        phv_reg = tf.nn.dropout(phv, 1 - (self.keep))\n",
        "        h_ = self.binomial_sampling(phv_reg)\n",
        "        return phv, h_\n",
        "\n",
        "    def sample_visible_units(self, h):\n",
        "        phi_h = tf.matmul(h, tf.transpose(a=self.w)) + self.bv\n",
        "        v_ = tf.nn.sigmoid(phi_h)\n",
        "        return v_\n",
        "\n",
        "    def gibbs_sampling(self):\n",
        "        self.v_k = self.v\n",
        "        for _ in range(self.k):\n",
        "            _, h_k = self.sample_hidden_units(self.v_k)\n",
        "            self.v_k = self.sample_visible_units(h_k)\n",
        "\n",
        "    def losses(self, vv):\n",
        "        with tf.compat.v1.variable_scope(\"losses\"):\n",
        "            obj = self.free_energy(vv) - self.free_energy(self.v_k)\n",
        "        return obj\n",
        "\n",
        "    def gibbs_protocol(self, i):\n",
        "        with tf.compat.v1.name_scope(\"gibbs_protocol\"):\n",
        "            epoch_percentage = (i / self.epochs) * 100\n",
        "            if epoch_percentage != 0:\n",
        "                if epoch_percentage >= self.sampling_protocol[self.l] and epoch_percentage <= self.sampling_protocol[self.l + 1]:\n",
        "                    self.k += 1\n",
        "                    self.l += 1\n",
        "                    self.gibbs_sampling()\n",
        "\n",
        "    def data_pipeline(self):\n",
        "        self.batch_size = tf.compat.v1.placeholder(tf.int64)\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices(self.vu)\n",
        "        self.dataset = self.dataset.shuffle(buffer_size=50, reshuffle_each_iteration=True, seed=self.seed)\n",
        "        self.dataset = self.dataset.batch(batch_size=self.batch_size).repeat()\n",
        "        self.iter = tf.compat.v1.data.make_initializable_iterator(self.dataset)\n",
        "        self.v = self.iter.get_next()\n",
        "\n",
        "    def init_metrics(self):\n",
        "        if self.with_metrics:\n",
        "            self.rmse = tf.sqrt(tf.compat.v1.losses.mean_squared_error(self.v, self.v_k, weights=tf.where(self.v > 0, 1, 0)))\n",
        "\n",
        "    def generate_graph(self):\n",
        "        log.info(\"Creating the computational graph\")\n",
        "        self.placeholder()\n",
        "        self.data_pipeline()\n",
        "        self.init_parameters()\n",
        "        log.info(\"Initialize Gibbs protocol\")\n",
        "        self.k = 1\n",
        "        self.l = 0\n",
        "        self.gibbs_sampling()\n",
        "        obj = self.losses(self.v)\n",
        "        rate = self.learning_rate / self.minibatch\n",
        "        self.opt = tf.compat.v1.train.AdamOptimizer(learning_rate=rate).minimize(loss=obj)\n",
        "\n",
        "    def init_gpu(self):\n",
        "        self.config_gpu = tf.compat.v1.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
        "        self.config_gpu.gpu_options.allow_growth = True\n",
        "\n",
        "    def init_training_session(self, xtr):\n",
        "        self.sess.run(self.iter.initializer, feed_dict={self.vu: xtr, self.batch_size: self.minibatch})\n",
        "        self.sess.run(tf.compat.v1.tables_initializer())\n",
        "\n",
        "    def batch_training(self, num_minibatches):\n",
        "        epoch_tr_err = 0\n",
        "        for _ in range(num_minibatches):\n",
        "            if self.with_metrics:\n",
        "                _, batch_err = self.sess.run([self.opt, self.rmse])\n",
        "                epoch_tr_err += batch_err / num_minibatches\n",
        "            else:\n",
        "                _ = self.sess.run(self.opt)\n",
        "        return epoch_tr_err\n",
        "\n",
        "    def fit(self, xtr):\n",
        "        self.init_training_session(xtr)\n",
        "        for epoch in tqdm(range(self.epochs), desc=\"Training\"):\n",
        "            self.gibbs_protocol(epoch)\n",
        "            tr_err = self.batch_training(num_minibatches=len(xtr) // self.minibatch)\n",
        "            if epoch % self.display_epoch == 0 or epoch == self.epochs - 1:\n",
        "                log.info(f\"Epoch {epoch}: Training error: {tr_err}\")\n",
        "\n",
        "    def predict(self, xtst):\n",
        "        self.sess.run(self.iter.initializer, feed_dict={self.vu: xtst, self.batch_size: xtst.shape[0]})\n",
        "        preds = self.sess.run(self.v_k)\n",
        "        return preds\n",
        "\n",
        "def calculate_precision(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate Precision@k for each user.\n",
        "\n",
        "    Args:\n",
        "    y_true: True ratings (2D numpy array)\n",
        "    y_pred: Predicted ratings (2D numpy array)\n",
        "    k: Number of top items to consider\n",
        "\n",
        "    Returns:\n",
        "    Mean Precision@k across all users\n",
        "    \"\"\"\n",
        "    precisions = []\n",
        "    for user_true, user_pred in zip(y_true, y_pred):\n",
        "        # Get indices of rated items in true ratings\n",
        "        relevant_items = np.where(user_true > 0)[0]\n",
        "\n",
        "        # If user has no relevant items, skip\n",
        "        if len(relevant_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get top k predicted items\n",
        "        recommended_items = np.argsort(user_pred)[::-1][:k]\n",
        "\n",
        "        # Calculate precision for this user\n",
        "        hits = np.isin(recommended_items, relevant_items)\n",
        "        precision = np.sum(hits) / k\n",
        "        precisions.append(precision)\n",
        "\n",
        "    return np.mean(precisions) if precisions else 0\n",
        "\n",
        "def calculate_map(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate Mean Average Precision@k.\n",
        "\n",
        "    Args:\n",
        "    y_true: True ratings (2D numpy array)\n",
        "    y_pred: Predicted ratings (2D numpy array)\n",
        "    k: Number of top items to consider\n",
        "\n",
        "    Returns:\n",
        "    MAP@k\n",
        "    \"\"\"\n",
        "    aps = []\n",
        "    for user_true, user_pred in zip(y_true, y_pred):\n",
        "        relevant_items = np.where(user_true > 0)[0]\n",
        "\n",
        "        if len(relevant_items) == 0:\n",
        "            continue\n",
        "\n",
        "        recommended_items = np.argsort(user_pred)[::-1][:k]\n",
        "\n",
        "        hits = np.isin(recommended_items, relevant_items)\n",
        "        precisions = np.cumsum(hits) / (np.arange(len(hits)) + 1)\n",
        "        ap = np.sum(precisions * hits) / min(k, len(relevant_items))\n",
        "        aps.append(ap)\n",
        "\n",
        "    return np.mean(aps) if aps else 0\n",
        "\n",
        "def calculate_recall(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate Recall@k for each user.\n",
        "\n",
        "    Args:\n",
        "    y_true: True ratings (2D numpy array)\n",
        "    y_pred: Predicted ratings (2D numpy array)\n",
        "    k: Number of top items to consider\n",
        "\n",
        "    Returns:\n",
        "    Mean Recall@k across all users\n",
        "    \"\"\"\n",
        "    recalls = []\n",
        "    for user_true, user_pred in zip(y_true, y_pred):\n",
        "        # Get indices of rated items in true ratings\n",
        "        relevant_items = np.where(user_true > 0)[0]\n",
        "\n",
        "        # If user has no relevant items, skip\n",
        "        if len(relevant_items) == 0:\n",
        "            continue\n",
        "\n",
        "        # Get top k predicted items\n",
        "        recommended_items = np.argsort(user_pred)[::-1][:k]\n",
        "\n",
        "        # Calculate recall for this user\n",
        "        hits = np.isin(recommended_items, relevant_items)\n",
        "        recall = np.sum(hits) / min(k, len(relevant_items))\n",
        "        recalls.append(recall)\n",
        "\n",
        "    return np.mean(recalls) if recalls else 0\n",
        "\n",
        "def calculate_ndcg(y_true, y_pred, k=5):\n",
        "    \"\"\"\n",
        "    Calculate NDCG@k for each user.\n",
        "\n",
        "    Args:\n",
        "    y_true: True ratings (2D numpy array)\n",
        "    y_pred: Predicted ratings (2D numpy array)\n",
        "    k: Number of top items to consider\n",
        "\n",
        "    Returns:\n",
        "    Mean NDCG@k across all users\n",
        "    \"\"\"\n",
        "    ndcgs = []\n",
        "    for user_true, user_pred in zip(y_true, y_pred):\n",
        "        # If user has no relevant items, skip\n",
        "        if np.sum(user_true > 0) == 0:\n",
        "            continue\n",
        "\n",
        "        ndcg = ndcg_score(user_true.reshape(1, -1), user_pred.reshape(1, -1), k=k)\n",
        "        ndcgs.append(ndcg)\n",
        "\n",
        "    return np.mean(ndcgs) if ndcgs else 0\n",
        "\n",
        "def hyperparameter_tuning(data, param_grid, test_size=0.2, random_state=42):\n",
        "    X_train, X_test = train_test_split(data, test_size=test_size, random_state=random_state)\n",
        "    best_params = None\n",
        "    best_ndcg = float('-inf')\n",
        "    for params in tqdm(ParameterGrid(param_grid), desc=\"Hyperparameter Tuning\"):\n",
        "        model = RBM(visible_units=data.shape[1], **params)\n",
        "        model.fit(X_train)\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        ndcg = calculate_ndcg(X_test, predictions, k=10)\n",
        "        recall = calculate_recall(X_test, predictions, k=10)\n",
        "        precision = calculate_precision(X_test, predictions, k=10)\n",
        "        map_score = calculate_map(X_test, predictions, k=10)\n",
        "\n",
        "        log.info(f\"Params: {params} - NDCG@10: {ndcg}, Recall@10: {recall}, Precision@10: {precision}, MAP@10: {map_score}\")\n",
        "        if ndcg > best_ndcg:\n",
        "            best_ndcg = ndcg\n",
        "            best_params = params\n",
        "            best_recall = recall\n",
        "            best_precision = precision\n",
        "            best_map = map_score\n",
        "    log.info(f\"Best Params: {best_params} - Best NDCG@10: {best_ndcg}, Best Recall@10: {best_recall}, Best Precision@10: {best_precision}, Best MAP@10: {best_map}\")\n",
        "    return best_params, best_ndcg, best_recall, best_precision, best_map\n",
        "\n",
        "def recommend(model, user_ratings, n=5):\n",
        "    \"\"\"\n",
        "    Generate recommendations for a user.\n",
        "\n",
        "    Args:\n",
        "    model: Trained RBM model\n",
        "    user_ratings: numpy array of user's ratings (1D array)\n",
        "    n: Number of recommendations to generate\n",
        "\n",
        "    Returns:\n",
        "    List of indices of recommended items\n",
        "    \"\"\"\n",
        "    # Reshape user_ratings to match the model input shape\n",
        "    user_ratings = user_ratings.reshape(1, -1)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = model.predict(user_ratings)\n",
        "\n",
        "    # Get indices of items the user hasn't rated\n",
        "    unrated_items = np.where(user_ratings[0] == 0)[0]\n",
        "\n",
        "    # Sort the predictions for unrated items\n",
        "    sorted_predictions = np.argsort(predictions[0][unrated_items])[::-1]\n",
        "\n",
        "    # Return the top n recommendations\n",
        "    return unrated_items[sorted_predictions[:n]]\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "    # Load your DataFrame\n",
        "    # Assuming df is your DataFrame with user-item ratings\n",
        "    # df = pd.read_csv('your_data.csv')  # Uncomment this line if you need to load the data\n",
        "\n",
        "    # Pivot the DataFrame to create a user-item matrix\n",
        "    #user_item_matrix = df.pivot(index='user_id', columns='item_id', values='rating').fillna(0)\n",
        "    user_item_matrix = df.fillna(0)\n",
        "    user_item_matrix.index = user_item_matrix.index.astype(int)\n",
        "    user_item_matrix.columns = user_item_matrix.columns.astype(str)\n",
        "\n",
        "\n",
        "    # Convert to numpy array\n",
        "    data = user_item_matrix.values\n",
        "\n",
        "    param_grid = {\n",
        "        'hidden_units': [100],\n",
        "        'keep_prob': [0.9],\n",
        "        'learning_rate': [0.005],\n",
        "        'minibatch_size': [64, 100],\n",
        "        'training_epoch': [75, 100]\n",
        "    }\n",
        "\n",
        "    best_params, best_ndcg, best_recall, best_precision, best_map = hyperparameter_tuning(data, param_grid)\n",
        "    print(f\"Best Params: {best_params}\")\n",
        "    print(f\"Best NDCG@5: {best_ndcg}\")\n",
        "    print(f\"Best Recall@5: {best_recall}\")\n",
        "    print(f\"Best Precision@5: {best_precision}\")\n",
        "    print(f\"Best MAP@5: {best_map}\")\n",
        "\n",
        "    # Train the final model with best parameters\n",
        "    final_model = RBM(visible_units=data.shape[1], **best_params)\n",
        "    final_model.fit(data)\n",
        "\n",
        "    # Example: Generate recommendations for the first user\n",
        "    user_ratings = data[0]\n",
        "    recommendations = recommend(final_model, user_ratings, n=5)\n",
        "    print(f\"Top 5 recommendations for user 0: {recommendations}\")\n",
        "\n",
        "    # Map item indices back to item IDs\n",
        "    item_ids = user_item_matrix.columns[recommendations]\n",
        "    print(f\"Recommended item IDs: {item_ids.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnBweL3mYAkR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}